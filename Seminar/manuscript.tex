\documentclass[10pt]{beamer}

\include{preamble}
%% Information (author, title, etc.) %%

\title[Short Title]{% short title for footer
    Separable Least-Mean Squares Beamforming 
    \vspace{0.5cm}
}

\author{Kenneth B. dos A. Benício}

\institute{
        \textit{Department of Teleinformatics Engineering}\\
        \textit{Federal University of Ceará}
        \vspace{0.5cm}
}
\date[Fortaleza, 2021]{% short date for footer
    Fortaleza, 2021
}


%% Content of slides %%
\begin{document}

%% Title slide
{
    \setbeamertemplate{footline}{}
    \setbeamertemplate{headline}{}
    \setbeamercolor{background canvas}{bg=oxfordblue}
    \maketitle
}


%% Contents slide
 \begin{frame}
 \frametitle{Outline}
 \tableofcontents
 \end{frame}

%% Including the slides
\setbeamercovered{transparent}

%% Problem Statement
\section{Problem Statement}
\begin{frame}{Problem Statement}
    \begin{block}{Objectives}
        \begin{enumerate}
            \item Recover a desired source signal by employing a large antenna array following an Uniform Rectangular Array (URA).
            \item Use spatial filter (beamforming) and optimize it according to the Mean square error (MSE) criterion.
            \item Solve the problem of slow convergence presented at LMS and NLMS algorithms.
        \end{enumerate}
    \end{block}
    \begin{block}{How to do so?}
        \begin{enumerate}
            \item Exploiting URA separability.
            \item Beamforming filter of the form $\mathbf{w} = \mathbf{w}_{v} \otimes \mathbf{w}_{h}$.
        \end{enumerate}
    \end{block}    
\end{frame}

%% Classic Filter
\section{Classic Filter Problems}
\begin{frame}[allowframebreaks]
    \frametitle{\insertsection}
    \begin{block}{The Classic Wiener Filter}
        \begin{itemize}
            \item The MSE minimization function
            \begin{align}
                J_{\text{MSE}}(\boldsymbol{w}) = \mathbb{E}\{ (s_{d}[k] - \boldsymbol{w}^{\text{H}} \boldsymbol{x}[k] )^{2} \} = 0,
            \end{align}
            \item The optimal wiener solution
            \begin{align}
                \boldsymbol{w}_{\text{opt}} = \boldsymbol{R}^{-1}_{x} \boldsymbol{p}_{xs},   
            \end{align}
            \item Problems with wiener filter
        \end{itemize}
    \end{block} 
    \framebreak
    \begin{block}{Stocasthic Gradient Filters}
        \begin{itemize}
            \item Received signal
            \begin{align}
                y[k] &= \boldsymbol{w}^{\text{H}} \boldsymbol{x}[k],
            \end{align}
            \item LMS Adaptative Filter
            \begin{align}
                \boldsymbol{w}[k+1] &= \boldsymbol{w}[k] + 2 \mu \boldsymbol{x}[k] e^{*}[k]
            \end{align}
            \item NLMS Adaptative Filter
            \begin{align}
                \boldsymbol{w}[k+1] &= \boldsymbol{w}[k] + \frac{\mu}{\gamma + \boldsymbol{x}^{\text{T}}[k] \boldsymbol{x}[k]} \boldsymbol{x}[k] e^{*}[k]
            \end{align}
            \item Advantages of Adaptative Filtering
        \end{itemize}
    \end{block} 
    \framebreak
    \begin{block}{The Matrix Kronecker Product}
        \begin{align}
            \mathbf{A} \otimes \mathbf{B} =
            \begin{bmatrix}
                a_{1,1} \mathbf{B} & a_{1,2} \mathbf{B} & \cdots & a_{1,J} \mathbf{B} \\
                a_{2,1} \mathbf{B} & a_{2,2} \mathbf{B} & \cdots & a_{2,J} \mathbf{B} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{I,1} \mathbf{B} & a_{I,2} \mathbf{B} & \cdots & a_{I,J} \mathbf{B}
            \end{bmatrix}
            \in \mathbb{C}^{RI \times JJ}.
        \end{align}
    \end{block}
\end{frame}

%% System Model
\section{System Model}
\begin{frame}[allowframebreaks]
    \frametitle{\insertsection}
    \begin{itemize}
        \item The received signal model follows a geometric channel
            \begin{align}
                \boldsymbol{x}[k] = \sum^{R}_{r = 1} \boldsymbol{a}(p_{r},q_{r}) s_{r}[k] + \boldsymbol{b}[k] = \boldsymbol{A} \boldsymbol{s}[k] + \boldsymbol{b}[k],
            \end{align}
        \item The vector $\boldsymbol{a}(p_{r},q_{r})$ represents an Uniform Rectangular Array (URA)
            \begin{align}
                \boldsymbol{a}(p_{r},q_{r}) &= \boldsymbol{a}_{v}(q_{r}) \otimes \boldsymbol{a}_{h}(p_{r}) \rightarrow a_{n}(p_{r},q_{r}) = a^{(h)}_{n_{h}} (p_r) a^{(v)}_{n_{v}} (q_r), \\
                a^{(h)}_{n_{h}} (p_r) &= e^{j \pi (n_{h} - 1)}p_{r}, \\
                a^{(v)}_{n_{v}} (q_r) &= e^{j \pi (n_{v} - 1)}q_{r},
            \end{align}
    \end{itemize}
    \begin{figure}
        \centering 
        \includegraphics[width=0.70\linewidth]{ura.png}
        \caption{Unifor Rectangular Array (URA) with $3 \times 3$ elements from \cite{ribeiroseparable}.}
        \label{fig:ura} 
    \end{figure}
\end{frame}

%% TLMS and ATLMS
\section{TLMS and ATLMS}
\begin{frame}[allowframebreaks]
    \frametitle{\insertsection}
    \begin{itemize}
        \item Filter Problem 
            \begin{align}
                \mathbb{E}\{ (s_{d}[k] - \boldsymbol{w}^{\text{H}} \boldsymbol{x}[k] )^{2} \} = 0
            \end{align}
        \item Tensor Filters
            \begin{align}
                y[k] = \left(\boldsymbol{w}_{v} \otimes \boldsymbol{w}_{h} \right)^{\text{H}} \boldsymbol{x}[k]
            \end{align}
        \item Tensor LMS
        \begin{align}
            \boldsymbol{w}_{h}[k+1] &= \boldsymbol{w}_{h}[k] + \mu[k] \boldsymbol{u}_{h}[k] e^{*}[k], \\
            \boldsymbol{w}_{v}[k+1] &= \boldsymbol{w}_{v}[k] + \mu[k] \boldsymbol{u}_{v}[k] e^{*}[k],
        \end{align}
        \item Alternating Tensor LMS 
    \end{itemize}
    \framebreak
    \begin{figure}
        \centering 
        \includegraphics[width=0.90\linewidth]{tlms.png}
        \caption{TLMS algorithm from \cite{ribeiroseparable}.}
        \label{fig:lms_alg} 
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[width=0.60\linewidth]{atlms.png}
        \caption{ATLMS algorithm from \cite{ribeiroseparable}.}
        \label{fig:atlms_alg} 
    \end{figure}
    \begin{block}{Convergence and Computational Complexity}
        \begin{itemize}
            \justifying
            \item The convergence for TLMS in MSE is
            
                \begin{align}
                    0 < \mu < \frac{2}{\left|\left|\boldsymbol{u}_{h}[k]\right|\right|^{2}_{2} + \left|\left|\boldsymbol{u}_{v}[k]\right|\right|^{2}_{2}}
                \end{align}
            
            \item The convergence for ATLMS in MSE is
            
                \begin{align}
                    0 < \mu < \frac{2}{\left|\left|\boldsymbol{u}_{i}[k]\right|\right|^{2}_{2}}, i \in \{h,v\},
                \end{align}
            
            \item TLMS and ATLMS has a computational complexity of $O(N_{h} + N_{v})$ and NLMS of $O(N)$. Since all the methods are linear in complexity
            the most important aspect that we must observe is the convergence rate.

        \end{itemize}
    \end{block}
\end{frame}

%% Simulation Results
\section{Simulation Results}
\begin{frame}
    \frametitle{Simulation Scenario}
    \begin{block}{Parameters}
        \begin{itemize}
            
            \item It was considered an URA of $4 \times 4$ antennas with $R = 4$ multipaths and QPSK information signals.
            
            \item The SNR was defined as $\text{SNR} = 1/\sigma^{2}_{b}$.

            \item We set as figure of merit the sample Mean Square Error (MSE) defined and calculated over $K = 10000$ samples
            
                \begin{align}
                    \text{MSE}(\boldsymbol{w}) = \frac{1}{K} \sum^{K}_{k = 1} \left|\left| s_{d}[k] - \boldsymbol{w}^{\text{H}} \boldsymbol{x}[k] \right|\right|^{2},
                \end{align}

        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{NLMS MSE Curve}
    \begin{figure}
        \centering
        \includegraphics[width=0.90\linewidth]{nlms_mse.png}
        \caption{Monter Carlo Experiment with 2500 runs for NLMS algorithm.}
        \label{fig:nlms} 
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{TLMS MSE Curve}
    \begin{figure}
        \centering
        \includegraphics[width=0.90\linewidth]{tlms_mse.png}
        \caption{Monter Carlo Experiment with 2500 runs for LMS algorithm.}
        \label{fig:tlms} 
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{ATLMS MSE Curve}
    \begin{figure}
        \centering
        \includegraphics[width=0.90\linewidth]{atlms_mse.png}
        \caption{Monter Carlo Experiment with 2500 runs for LMS algorithm.}
        \label{fig:atlms} 
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{ATLMS: Different sampling intervals}
    \begin{figure}
        \centering
        \includegraphics[width=0.90\linewidth]{atlms_sampling.png}
        \caption{Monter Carlo Experiment with 2500 runs for the ATLMS with different sampling intervals.}
        \label{fig:atlms_sampling} 
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Processing Time: TLMS vs. ATLMS}
    \begin{figure}
        \centering
        \includegraphics[width=0.90\linewidth]{atlms_time.png}
        \caption{Run time process for ATLMS with different sampling intervals.}
        \label{fig:atlms_time} 
    \end{figure}
\end{frame}

%% Conclusion
\section{Conclusion}
\begin{frame}[allowframebreaks]
    \frametitle{\insertsection}
    \begin{itemize}
        \item TLMS and ATLMS algorithms converges faster than the traditional approachs using NLMS.
        \item TLMS and ATLMS converges to almost the same end, however ATLMS has a greater misadjustment error at the end.
        \item ATLMS can be slightly faster than the TLMS.
    \end{itemize}
\end{frame}

%% References
\section*{References}
\begin{frame} 
    \frametitle{\insertsection}
    \bibliographystyle{ieeetr}
    \bibliography{references}
\end{frame}

%% End of Presentation
\begin{frame}
    \begin{center}
        \Huge Thank you for your presence!
    \end{center}
\end{frame}

\end{document}